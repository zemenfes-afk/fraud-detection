{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-27T16:01:14.591779Z",
     "start_time": "2025-12-27T16:01:14.575351Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Force the path to the root\n",
    "project_root = r'C:\\Users\\HP\\PycharmProjects\\fraud-detection'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Check if we can see the file itself\n",
    "file_path = os.path.join(project_root, 'src', 'preprocessing.py')\n",
    "print(f\"Checking for file at: {file_path}\")\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "try:\n",
    "    from src.preprocessing import merge_fraud_with_countries\n",
    "    print(\"✅ ✅ IMPORT SUCCESSFUL!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Still failing. Error: {e}\")\n",
    "    print(\"Logic: We will define the function locally to proceed with Task 1.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for file at: C:\\Users\\HP\\PycharmProjects\\fraud-detection\\src\\preprocessing.py\n",
      "File exists: False\n",
      "❌ Still failing. Error: No module named 'src'\n",
      "Logic: We will define the function locally to proceed with Task 1.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:03:17.098022Z",
     "start_time": "2025-12-27T16:02:44.846768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "# Load datasets\n",
    "fraud_data = pd.read_csv('../data/Fraud_Data.csv')\n",
    "ip_to_country = pd.read_csv('../data/IpAddress_to_Country.csv')\n",
    "\n",
    "# Handle missing values and duplicates\n",
    "fraud_data = fraud_data.drop_duplicates()\n",
    "print(f\"Initial Fraud Data Shape: {fraud_data.shape}\")\n",
    "print(f\"Missing Values:\\n{fraud_data.isnull().sum()}\")\n",
    "\n",
    "# Correct data types for timestamps\n",
    "fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time'])\n",
    "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])"
   ],
   "id": "abc520b7271d5d9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Fraud Data Shape: (151112, 11)\n",
      "Missing Values:\n",
      "user_id           0\n",
      "signup_time       0\n",
      "purchase_time     0\n",
      "purchase_value    0\n",
      "device_id         0\n",
      "source            0\n",
      "browser           0\n",
      "sex               0\n",
      "age               0\n",
      "ip_address        0\n",
      "class             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T15:59:52.580585Z",
     "start_time": "2025-12-27T15:59:52.575796Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3f38863d6b99e1fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:16:26.208374Z",
     "start_time": "2025-12-27T16:16:25.399433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "# 1. Load Data\n",
    "fraud_data = pd.read_csv('../data/Fraud_Data.csv')\n",
    "ip_to_country = pd.read_csv('../data/IpAddress_to_Country.csv')\n",
    "\n",
    "# --- INTELLIGENT PREPROCESSING ---\n",
    "\n",
    "def ip_to_int(ip):\n",
    "    \"\"\"Converts IPv4 string to integer. Returns None if it fails.\"\"\"\n",
    "    try:\n",
    "        return struct.unpack(\"!I\", socket.inet_aton(ip))[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# CHECK: Is the IP address already a number?\n",
    "# Many datasets already come with IPs converted to floats.\n",
    "if np.issubdtype(fraud_data['ip_address'].dtype, np.number):\n",
    "    print(\"ℹ️ IP addresses are already numeric. Skipping socket conversion.\")\n",
    "    fraud_data['ip_address_int'] = fraud_data['ip_address']\n",
    "else:\n",
    "    print(\"ℹ️ IP addresses are strings (e.g., '192.168.1.1'). Converting...\")\n",
    "    fraud_data['ip_address_int'] = fraud_data['ip_address'].apply(ip_to_int)\n",
    "\n",
    "# 2. Handle Missing Values\n",
    "# Drop rows only if the IP is truly missing/invalid\n",
    "fraud_data = fraud_data.dropna(subset=['ip_address_int'])\n",
    "ip_to_country = ip_to_country.dropna(subset=['lower_bound_ip_address'])\n",
    "\n",
    "# 3. Force Integer Types\n",
    "fraud_data['ip_address_int'] = fraud_data['ip_address_int'].astype('int64')\n",
    "ip_to_country['lower_bound_ip_address'] = ip_to_country['lower_bound_ip_address'].astype('int64')\n",
    "ip_to_country['upper_bound_ip_address'] = ip_to_country['upper_bound_ip_address'].astype('int64')\n",
    "\n",
    "# 4. Sort and Merge\n",
    "fraud_data = fraud_data.sort_values('ip_address_int')\n",
    "ip_to_country = ip_to_country.sort_values('lower_bound_ip_address')\n",
    "\n",
    "print(f\"Merging {len(fraud_data)} fraud records...\")\n",
    "merged_data = pd.merge_asof(\n",
    "    fraud_data,\n",
    "    ip_to_country,\n",
    "    left_on='ip_address_int',\n",
    "    right_on='lower_bound_ip_address'\n",
    ")\n",
    "\n",
    "# 5. Correct the 'Country' Logic (Keep strictly valid matches)\n",
    "# merge_asof gives the closest match; we must ensure it's within the upper bound\n",
    "merged_data['country'] = np.where(\n",
    "    merged_data['ip_address_int'] <= merged_data['upper_bound_ip_address'],\n",
    "    merged_data['country'],\n",
    "    'Unknown'\n",
    ")\n",
    "\n",
    "print(f\"✅ Merge Successful! Final Shape: {merged_data.shape}\")\n",
    "print(f\"Top Countries:\\n{merged_data['country'].value_counts().head()}\")"
   ],
   "id": "5f530975c4e057a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ IP addresses are already numeric. Skipping socket conversion.\n",
      "Merging 151112 fraud records...\n",
      "✅ Merge Successful! Final Shape: (151112, 15)\n",
      "Top Countries:\n",
      "country\n",
      "United States     58049\n",
      "Unknown           21966\n",
      "China             12038\n",
      "Japan              7306\n",
      "United Kingdom     4490\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3257d4ada12805ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Step 3: Feature Engineering & Data Transformation",
   "id": "b2264ac0006fe881"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:26:58.514680Z",
     "start_time": "2025-12-27T16:26:57.761243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Force Convert Columns to Datetime (Fixes the TypeError)\n",
    "print(\"Converting timestamps to datetime objects...\")\n",
    "merged_data['purchase_time'] = pd.to_datetime(merged_data['purchase_time'])\n",
    "merged_data['signup_time'] = pd.to_datetime(merged_data['signup_time'])\n",
    "\n",
    "# 2. Re-run Feature Engineering\n",
    "print(\"calculating time features...\")\n",
    "# Now the subtraction will work because they are dates, not strings\n",
    "merged_data['time_since_signup'] = (merged_data['purchase_time'] - merged_data['signup_time']).dt.total_seconds()\n",
    "merged_data['hour_of_day'] = merged_data['purchase_time'].dt.hour\n",
    "merged_data['day_of_week'] = merged_data['purchase_time'].dt.dayofweek\n",
    "\n",
    "# 3. Velocity Feature\n",
    "merged_data['device_usage_count'] = merged_data.groupby('device_id')['device_id'].transform('count')\n",
    "\n",
    "# 4. Final Check\n",
    "print(\"✅ Feature Engineering Complete!\")\n",
    "print(merged_data[['time_since_signup', 'hour_of_day']].head())"
   ],
   "id": "152e374cdcf2946e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting timestamps to datetime objects...\n",
      "calculating time features...\n",
      "✅ Feature Engineering Complete!\n",
      "   time_since_signup  hour_of_day\n",
      "0          1763014.0           10\n",
      "1          1084823.0           17\n",
      "2           749320.0            8\n",
      "3          7434634.0           21\n",
      "4          1407619.0            7\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:28:40.034391Z",
     "start_time": "2025-12-27T16:28:39.631783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1. HANDLE HIGH CARDINALITY (Country) ---\n",
    "# We keep the top 10 countries and label the rest as 'Other'\n",
    "# This prevents creating 180+ columns which would slow down the model\n",
    "top_countries = merged_data['country'].value_counts().index[:10]\n",
    "merged_data['country_processed'] = merged_data['country'].apply(lambda x: x if x in top_countries else 'Other')\n",
    "\n",
    "# --- 2. ONE-HOT ENCODING ---\n",
    "# Convert text categories (Source, Browser, Sex, Country) into binary columns (0 or 1)\n",
    "columns_to_encode = ['source', 'browser', 'sex', 'country_processed']\n",
    "# We use drop_first=True to reduce redundancy (e.g., if not Male, it's Female)\n",
    "df_encoded = pd.get_dummies(merged_data, columns=columns_to_encode, drop_first=True)\n",
    "\n",
    "# --- 3. DROP UNNECESSARY COLUMNS ---\n",
    "# Remove ID columns and original timestamps (we already extracted the useful info)\n",
    "cols_to_drop = ['user_id', 'device_id', 'ip_address', 'ip_address_int',\n",
    "                'signup_time', 'purchase_time', 'country',\n",
    "                'lower_bound_ip_address', 'upper_bound_ip_address']\n",
    "df_model = df_encoded.drop(columns=cols_to_drop)\n",
    "\n",
    "# --- 4. SEPARATE FEATURES (X) AND TARGET (y) ---\n",
    "X = df_model.drop(columns=['class'])\n",
    "y = df_model['class']\n",
    "\n",
    "print(f\"✅ Data Transformation Complete!\")\n",
    "print(f\"Final Features shape: {X.shape}\")\n",
    "print(f\"Features list: {list(X.columns[:10])}...\") # Preview first 10 columns"
   ],
   "id": "84cc3efa2c91c09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Transformation Complete!\n",
      "Final Features shape: (151112, 23)\n",
      "Features list: ['purchase_value', 'age', 'time_since_signup', 'hour_of_day', 'day_of_week', 'device_usage_count', 'source_Direct', 'source_SEO', 'browser_FireFox', 'browser_IE']...\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:31:26.478933Z",
     "start_time": "2025-12-27T16:31:24.636501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- 1. TRAIN-TEST SPLIT ---\n",
    "# We use 'stratify=y' to ensure both train and test sets have the same % of fraud\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 2. SCALING (StandardScaler) ---\n",
    "# Fit ONLY on training data, then transform both\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 3. APPLY SMOTE (Training Data Only) ---\n",
    "print(\"Applying SMOTE to handle imbalance...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# --- 4. VERIFY RESULTS ---\n",
    "print(\"-\" * 30)\n",
    "print(f\"Original Training Shape: {X_train.shape}\")\n",
    "print(f\"Original Fraud Count: {sum(y_train == 1)}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Resampled Training Shape: {X_train_resampled.shape}\")\n",
    "print(f\"Resampled Fraud Count: {sum(y_train_resampled == 1)}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"✅ Data is split, scaled, and balanced. Ready for Model Building!\")\n",
    "# Save the processed data for Task 2\n",
    "print(\"Saving processed data...\")\n",
    "# combining X and y just for saving\n",
    "processed_data = pd.DataFrame(X, columns=X.columns)\n",
    "processed_data['class'] = y\n",
    "\n",
    "# Save to the 'processed' folder\n",
    "processed_data.to_csv('../data/processed/fraud_data_processed.csv', index=False)\n",
    "print(\"✅ File saved to 'data/processed/fraud_data_processed.csv'\")"
   ],
   "id": "a62209003894222c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "Applying SMOTE to handle imbalance...\n",
      "------------------------------\n",
      "Original Training Shape: (120889, 23)\n",
      "Original Fraud Count: 11321\n",
      "------------------------------\n",
      "Resampled Training Shape: (219136, 23)\n",
      "Resampled Fraud Count: 109568\n",
      "------------------------------\n",
      "✅ Data is split, scaled, and balanced. Ready for Model Building!\n",
      "Saving processed data...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\data\\processed'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 39\u001B[39m\n\u001B[32m     36\u001B[39m processed_data[\u001B[33m'\u001B[39m\u001B[33mclass\u001B[39m\u001B[33m'\u001B[39m] = y\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# Save to the 'processed' folder\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m \u001B[43mprocessed_data\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m../data/processed/fraud_data_processed.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m✅ File saved to \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata/processed/fraud_data_processed.csv\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\fraud-detection\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001B[39m, in \u001B[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    327\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) > num_allow_args:\n\u001B[32m    328\u001B[39m     warnings.warn(\n\u001B[32m    329\u001B[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001B[32m    330\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m    331\u001B[39m         stacklevel=find_stack_level(),\n\u001B[32m    332\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\fraud-detection\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3989\u001B[39m, in \u001B[36mNDFrame.to_csv\u001B[39m\u001B[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[39m\n\u001B[32m   3978\u001B[39m df = \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.to_frame()\n\u001B[32m   3980\u001B[39m formatter = DataFrameFormatter(\n\u001B[32m   3981\u001B[39m     frame=df,\n\u001B[32m   3982\u001B[39m     header=header,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3986\u001B[39m     decimal=decimal,\n\u001B[32m   3987\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m3989\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3990\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3991\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3992\u001B[39m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[43m=\u001B[49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3993\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3994\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3995\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3996\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3997\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3998\u001B[39m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3999\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4000\u001B[39m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4001\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4002\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4003\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4004\u001B[39m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4005\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4006\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\fraud-detection\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001B[39m, in \u001B[36mDataFrameRenderer.to_csv\u001B[39m\u001B[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[39m\n\u001B[32m    993\u001B[39m     created_buffer = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    995\u001B[39m csv_formatter = CSVFormatter(\n\u001B[32m    996\u001B[39m     path_or_buf=path_or_buf,\n\u001B[32m    997\u001B[39m     lineterminator=lineterminator,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1012\u001B[39m     formatter=\u001B[38;5;28mself\u001B[39m.fmt,\n\u001B[32m   1013\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m \u001B[43mcsv_formatter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[32m   1017\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\fraud-detection\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001B[39m, in \u001B[36mCSVFormatter.save\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    247\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03mCreate the writer & save.\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    250\u001B[39m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[32m    259\u001B[39m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[32m    260\u001B[39m     \u001B[38;5;28mself\u001B[39m.writer = csvlib.writer(\n\u001B[32m    261\u001B[39m         handles.handle,\n\u001B[32m    262\u001B[39m         lineterminator=\u001B[38;5;28mself\u001B[39m.lineterminator,\n\u001B[32m   (...)\u001B[39m\u001B[32m    267\u001B[39m         quotechar=\u001B[38;5;28mself\u001B[39m.quotechar,\n\u001B[32m    268\u001B[39m     )\n\u001B[32m    270\u001B[39m     \u001B[38;5;28mself\u001B[39m._save()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\fraud-detection\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:749\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    747\u001B[39m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[32m    748\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[32m--> \u001B[39m\u001B[32m749\u001B[39m     \u001B[43mcheck_parent_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    751\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[32m    752\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m compression != \u001B[33m\"\u001B[39m\u001B[33mzstd\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    753\u001B[39m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\fraud-detection\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:616\u001B[39m, in \u001B[36mcheck_parent_directory\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    614\u001B[39m parent = Path(path).parent\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent.is_dir():\n\u001B[32m--> \u001B[39m\u001B[32m616\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[33mrf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCannot save file into a non-existent directory: \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mOSError\u001B[39m: Cannot save file into a non-existent directory: '..\\data\\processed'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:32:59.229904Z",
     "start_time": "2025-12-27T16:32:57.764295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# 1. Define the file path\n",
    "save_path = '../data/processed/fraud_data_processed.csv'\n",
    "\n",
    "# 2. Check if the directory exists; if not, create it!\n",
    "# os.path.dirname(save_path) gets \"../data/processed\"\n",
    "directory = os.path.dirname(save_path)\n",
    "if not os.path.exists(directory):\n",
    "    print(f\"Directory '{directory}' not found. Creating it now...\")\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# 3. Save the file\n",
    "print(\"Saving processed data...\")\n",
    "processed_data.to_csv(save_path, index=False)\n",
    "print(f\"✅ File successfully saved to '{save_path}'\")"
   ],
   "id": "13b472bda2987426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../data/processed' not found. Creating it now...\n",
      "Saving processed data...\n",
      "✅ File successfully saved to '../data/processed/fraud_data_processed.csv'\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:35:00.903543Z",
     "start_time": "2025-12-27T16:34:59.110091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "# 1. Load the processed data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/processed/fraud_data_processed.csv')\n",
    "\n",
    "# 2. Separate Features (X) and Target (y)\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "\n",
    "# 3. Stratified Train-Test Split (80/20)\n",
    "# Stratify ensures we have the same proportion of fraud in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 4. Scale the Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Apply SMOTE (Only to Training Data!)\n",
    "print(\"Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"✅ Data Ready. Training Shape: {X_train_res.shape}\")"
   ],
   "id": "3200786b037ed85b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Applying SMOTE...\n",
      "✅ Data Ready. Training Shape: (219136, 23)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T16:35:43.983864Z",
     "start_time": "2025-12-27T16:35:42.294555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "# 1. Load the processed data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/processed/fraud_data_processed.csv')\n",
    "\n",
    "# 2. Separate Features (X) and Target (y)\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "\n",
    "# 3. Stratified Train-Test Split (80/20)\n",
    "# Stratify ensures we have the same proportion of fraud in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 4. Scale the Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Apply SMOTE (Only to Training Data!)\n",
    "print(\"Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"✅ Data Ready. Training Shape: {X_train_res.shape}\")"
   ],
   "id": "b57dfb622368e3c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Applying SMOTE...\n",
      "✅ Data Ready. Training Shape: (219136, 23)\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
